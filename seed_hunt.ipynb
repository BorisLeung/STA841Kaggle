{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borisleung/anaconda3/envs/sta841kaggle/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "from data import Y_COLUMNS, combined_train_with_num_pov\n",
    "\n",
    "\n",
    "X, y_binarized, y = (\n",
    "    combined_train_with_num_pov.drop(Y_COLUMNS + [\"num_pov\"], axis=1),\n",
    "    combined_train_with_num_pov[Y_COLUMNS],\n",
    "    combined_train_with_num_pov[\"num_pov\"],\n",
    ")\n",
    "X = X.drop([\"house_q10\"], axis=1)\n",
    "X = X.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data import get_preprocessor\n",
    "\n",
    "MAX_ITER = 1000\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "seed_search_range = range(1000)\n",
    "best_values = []\n",
    "for SEED in seed_search_range:\n",
    "\n",
    "    def objective(trial: optuna.Trial):\n",
    "        null_threshold = trial.suggest_float(\"null_threshold\", 1e-30, 0.5)\n",
    "        C = trial.suggest_float(\"C\", 1e-10, 1e10, log=True)\n",
    "        imputer_strategy = trial.suggest_categorical(\n",
    "            \"imputer_strategy\", [\"mean\", \"median\", \"most_frequent\"]\n",
    "        )\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"])\n",
    "        intercept_scaling = trial.suggest_float(\n",
    "            \"intercept_scaling\", 1e-10, 1e10, log=True\n",
    "        )\n",
    "\n",
    "        dropped_columns = X.columns[X.isnull().mean() > null_threshold]\n",
    "        X_cleaned = X.drop(dropped_columns, axis=1)\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid, y_train_binarized, y_valid_binarized = (\n",
    "            train_test_split(\n",
    "                X_cleaned, y, y_binarized, test_size=1 / 5, random_state=SEED\n",
    "            )\n",
    "        )\n",
    "\n",
    "        preprocessor = get_preprocessor(\n",
    "            imputer_strategy=[\n",
    "                \"most_frequent\",\n",
    "                \"most_frequent\",\n",
    "                imputer_strategy,\n",
    "                imputer_strategy,\n",
    "            ],\n",
    "            remainder=\"drop\",\n",
    "        )\n",
    "        X_train = preprocessor.fit_transform(X_train)\n",
    "        X_valid = preprocessor.transform(X_valid)\n",
    "\n",
    "        model = LinearSVC(\n",
    "            C=C,\n",
    "            penalty=penalty,\n",
    "            intercept_scaling=intercept_scaling,\n",
    "            max_iter=MAX_ITER,\n",
    "            random_state=SEED,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        calibration_method = trial.suggest_categorical(\n",
    "            \"calibration_method\", [\"sigmoid\", \"isotonic\"]\n",
    "        )\n",
    "        calibrated_model = CalibratedClassifierCV(\n",
    "            model, cv=\"prefit\", method=calibration_method\n",
    "        )\n",
    "        calibrated_model.fit(X_train, y_train)\n",
    "        y_pred = calibrated_model.predict_proba(X_valid)\n",
    "        valid_loss = log_loss(y_valid_binarized, y_pred, normalize=False) / len(\n",
    "            y_valid_binarized\n",
    "        )\n",
    "        return valid_loss\n",
    "\n",
    "    try:\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=50, n_jobs=-1, show_progress_bar=True)\n",
    "        best_values.append(study.best_value)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        best_values.append(np.nan)\n",
    "best_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([662,  16, 713, 850, 183, 391, 957,  68, 934, 264, 746, 259, 599,\n",
       "        99, 177, 589, 590, 436,  84, 396, 635, 772, 853, 893, 379, 628,\n",
       "       704,  93, 303,  21, 657,  61, 193,  34, 640, 666, 380, 705, 583,\n",
       "       197, 272, 369, 784, 919, 722, 322, 239, 311, 360, 608, 899, 701,\n",
       "       626, 180, 834, 975, 209, 331, 792, 869, 907,  72, 910, 656, 109,\n",
       "       691, 769, 450, 502, 808, 849, 566, 775, 925, 319, 820, 539, 206,\n",
       "       283, 670, 524, 201, 310, 913, 660, 718, 564, 500, 515, 806, 731,\n",
       "       174,  20, 456, 165, 372, 527, 813, 293, 672, 522, 683, 223, 240,\n",
       "       410, 572, 544, 440, 428, 614, 685, 147, 489, 478, 409, 431, 888,\n",
       "       543, 645, 711, 994, 616, 176, 190, 411, 858, 728,  71, 842, 421,\n",
       "       375, 674, 312, 213, 881,  45, 280, 802, 953,  69, 346, 901, 156,\n",
       "       827,  81, 279, 437, 631,  78, 255, 359, 702, 673, 821, 896, 172,\n",
       "       852, 444,  19, 121, 946, 818, 173, 234, 476, 400, 313, 452, 963,\n",
       "       191, 286, 203, 250, 422, 999, 574, 637, 166, 636, 164, 343, 886,\n",
       "       187, 795, 122, 884, 944, 219, 149, 516,  89, 249, 831, 680, 805,\n",
       "       790, 386, 238, 991, 814,  88, 328, 140, 725,  82, 423, 487, 118,\n",
       "        76, 304, 161, 358, 344,  30, 229, 754, 222, 432, 463, 245, 838,\n",
       "       290, 890, 833, 597,  32, 362, 282, 399, 607, 817, 340, 202, 866,\n",
       "       947, 816, 545,  41, 926, 457, 750, 225, 930, 932, 675, 961, 384,\n",
       "       367, 977, 252, 528,   6, 493, 764, 119, 464,  55, 160, 356, 368,\n",
       "       198, 770, 779,  91, 427,  17, 557, 830, 563,  35,  92, 851, 707,\n",
       "        42, 371, 762, 314, 804, 345, 826, 270, 787, 325, 153, 846, 584,\n",
       "       416, 305, 373, 822, 668,  36, 247, 908, 302,  66, 144, 127, 855,\n",
       "       958, 706, 370, 684, 771, 208, 594, 175,  11, 295, 734,  50, 296,\n",
       "       185, 287, 284, 885, 513, 192, 554, 350, 950, 648, 212, 491, 595,\n",
       "       664, 485, 717, 579, 458, 927, 154, 388,  15, 194, 868, 382, 929,\n",
       "        38,  53, 170, 329, 936, 980, 736, 875, 877, 863, 465, 677, 569,\n",
       "       856, 625, 571,  14,   3, 196,  80,  57, 622, 768, 389, 159, 763,\n",
       "       532, 615, 499, 519, 782, 689, 577, 807, 800, 251, 970,  26, 477,\n",
       "        96, 879,   1, 274, 298, 588, 914, 714, 964, 230, 916, 823, 952,\n",
       "       514, 517, 681, 690, 179, 692, 591, 365, 873,  18, 486, 793, 939,\n",
       "       107, 619, 553, 686, 989, 473, 393, 508, 136, 115,  73, 189, 724,\n",
       "       254, 181, 737, 920, 214, 809, 894, 260, 562, 336, 497, 854, 481,\n",
       "         2, 511, 745,  48, 445, 128, 651, 261, 224, 742, 945, 633, 155,\n",
       "       688, 535, 523, 942, 521, 327, 829, 253, 467, 518, 395,  13, 900,\n",
       "       719, 988, 267, 565, 653, 460, 694, 931, 788, 857, 781, 757, 940,\n",
       "       634, 318, 665,  63, 145, 347, 354, 723, 398, 330, 472, 135, 895,\n",
       "       889, 113, 695, 729, 268, 138, 708, 624, 968, 131, 904, 323, 922,\n",
       "       361, 355, 839, 397, 412, 845, 623, 561, 699, 650, 506, 791, 248,\n",
       "       357, 641, 446, 509, 451, 760, 537, 960, 130, 696, 874, 281, 430,\n",
       "        51, 606, 533, 316, 102, 466, 178, 498, 475, 658, 937, 546,  31,\n",
       "       534, 778, 547, 439,   5,  47, 815, 333, 377, 810, 575, 667, 117,\n",
       "       182, 378, 981, 158, 106, 278, 104, 462, 308, 232, 536, 448, 420,\n",
       "       163, 859, 576, 976, 246, 114, 507, 549, 129, 150, 812, 479, 990,\n",
       "       488, 738,  39, 146, 342, 218, 168, 257, 540, 548, 321, 285, 780,\n",
       "       120, 700, 996, 211, 663, 570, 639, 266, 231, 492, 954,  70,  29,\n",
       "       882,  97, 510, 438,  65, 503, 697, 755, 461,   4, 217, 387, 836,\n",
       "       100, 721, 928, 529, 832, 504, 971, 652, 469, 123, 840, 789, 794,\n",
       "       236, 151, 111, 761, 407, 871, 819, 646, 494, 898, 531, 974, 726,\n",
       "       116, 552, 555, 441, 417, 394, 644,  43, 935, 580, 843,  49, 710,\n",
       "       137, 732, 759, 339, 661, 105, 676, 435, 598, 949, 880, 783, 237,\n",
       "       559, 596, 401, 785, 609, 630, 205, 148, 294, 654, 578, 959, 263,\n",
       "       915, 348, 798, 538, 943, 938, 297,  75, 825, 752, 828, 110, 337,\n",
       "       613, 112, 415, 501, 776, 167, 449, 632, 332, 978, 408, 276, 610,\n",
       "       647,  54, 169, 774, 865,  12, 414,  37, 125, 573, 374, 442, 291,\n",
       "       905, 801, 447,  64, 679, 765,  25, 841, 455, 753, 847, 992, 468,\n",
       "       867,  77, 924, 496, 872, 418, 618, 560, 141, 716, 848, 204, 909,\n",
       "       602, 228,  46, 512, 256, 740, 709, 262, 226, 157, 906, 649, 870,\n",
       "       142, 207, 844, 918, 612, 216, 643, 126, 669, 803, 703,  58, 199,\n",
       "       558, 459, 917, 385, 277, 390, 603, 299, 585, 993, 582, 758, 275,\n",
       "       891, 747,   9, 426,  59, 617, 338, 306, 911, 735,  86, 324, 586,\n",
       "       835, 797, 195, 998, 505, 317, 184, 402, 495, 979,  33, 392, 474,\n",
       "       965, 951, 966,   7, 956, 351, 434, 883,  60,  52, 739, 811, 727,\n",
       "       864,  79, 108, 604, 969, 751, 315, 967, 470, 171, 741, 530, 443,\n",
       "       273, 134, 483, 480, 376, 743, 103, 525, 541, 987, 233, 241, 786,\n",
       "       593, 605, 289, 215, 876,  56, 235,  95, 983, 642, 433, 698, 556,\n",
       "        85, 902, 335, 878, 799, 101, 326,   8, 744,  23, 288, 627, 490,\n",
       "       629, 186, 542, 381, 712, 244, 749, 162, 973, 773,  62, 600, 837,\n",
       "       941, 453, 143, 982, 587, 824, 405, 912, 300, 424, 730, 986, 271,\n",
       "       678, 482, 581, 903,  22,  87, 862, 200, 403, 132, 985, 139, 715,\n",
       "         0, 152, 341, 767, 349, 860, 429,  24, 307, 567, 258, 655, 210,\n",
       "       659, 364, 962, 520, 269, 997, 671, 352, 188,  10, 995, 265, 638,\n",
       "       292, 551, 550, 984, 568, 363, 243, 242, 592, 601, 611, 620, 621,\n",
       "       227, 221, 972, 220, 353, 484, 682, 897, 366, 796, 425, 419,  98,\n",
       "        94,  90,  40,  83, 921, 413,  44, 334, 861,  67, 406, 887, 892,\n",
       "       404,  74, 955, 320, 124, 471, 687, 693, 301, 383, 720, 454, 309,\n",
       "       923, 733, 756, 526, 933,  27,  28, 133, 766, 777, 748, 948])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(best_values).argsort()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sta841kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
